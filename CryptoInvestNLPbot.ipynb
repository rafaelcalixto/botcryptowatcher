{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começamos realizando o load de algumas bibliotecas auxiliares que iremos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load as jsload\n",
    "from tweepy import OAuthHandler, API\n",
    "from warnings import catch_warnings, filterwarnings\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora importamos as funções do spacy que iremos utilizar e carregar o modelo pré treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import load\n",
    "\n",
    "nlp = load(\"pt_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloquei as chaves de acesso a minha conta no Twitter em um arquivo separado :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"key.json\") as file:\n",
    "    keys = jsload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(\n",
    "    consumer_key = keys[\"cunsumer_key\"],\n",
    "    consumer_secret = keys[\"consumer_secret\"]\n",
    ")\n",
    "\n",
    "auth.set_access_token(\n",
    "    key = keys[\"access_token\"],\n",
    "    secret = keys[\"access_token_secret\"]\n",
    ")\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos baixar os dados do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturando dados do Twitter\n",
    "tweets = {each._json[\"user\"][\"name\"] : each._json[\"text\"]  for each in api.search(q = \"#bitcoin\", lang = \"pt\", count = 200)}\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos criar uma função que realize o filtro de palavras que desejamos trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(word, cut_stop = True):\n",
    "    if word.is_stop and cut_stop:\n",
    "        return False\n",
    "    elif word.is_punct:\n",
    "        return False\n",
    "    elif word.suffix_ == \"…\":\n",
    "        return False\n",
    "    elif word.like_url:\n",
    "        return False\n",
    "    elif word.like_email:\n",
    "        return False\n",
    "    elif word.like_num:\n",
    "        return False\n",
    "    elif word.prefix_ == \"@\":\n",
    "        return False\n",
    "    elif word.text in [\" \", \"\\n\", \"\\n\\n\", \"...\", 'RT']:\n",
    "        return False\n",
    "    elif not word.text.isalnum():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o pré processamento das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento: Stop Words e Lemmatazing\n",
    "processeded = []\n",
    "# interando sobre cada tweet\n",
    "for user, tweet in tweets.items():\n",
    "    row = []\n",
    "    for word in nlp(tweet): # este é o pipeline\n",
    "        # filtrando as palavras\n",
    "        if word_filter(word):\n",
    "            # após selecionar as palavras, é adicionado o seu formato lematizado\n",
    "            lemm = nlp.vocab[word.text]\n",
    "            row.append(lemm.text)\n",
    "    print(f\"{user} : {row}\")\n",
    "    processeded.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processeded = []\n",
    "ner = []\n",
    "adj = []\n",
    "for each in tweets.values():\n",
    "    doc = nlp(each)\n",
    "    processeded.append([nlp.vocab[word.text].text for word in doc if word_filter(word)])\n",
    "    ner.append([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
