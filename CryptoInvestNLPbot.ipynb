{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começamos realizando o load de algumas bibliotecas auxiliares que iremos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load as jsload\n",
    "from tweepy import OAuthHandler, API\n",
    "from warnings import catch_warnings, filterwarnings\n",
    "from random import shuffle\n",
    "from time import strftime, mktime\n",
    "from datetime import datetime\n",
    "from emoji import get_emoji_regexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora importamos as funções do spacy que iremos utilizar e carregar o modelo pré treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import load\n",
    "\n",
    "nlp = load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloquei as chaves de acesso a minha conta no Twitter em um arquivo separado :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"key.json\") as file:\n",
    "    keys = jsload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(\n",
    "    consumer_key = keys[\"API key\"],\n",
    "    consumer_secret = keys[\"API secret key\"]\n",
    ")\n",
    "\n",
    "auth.set_access_token(\n",
    "    key = keys[\"Access token\"],\n",
    "    secret = keys[\"Access token secret\"]\n",
    ")\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caracters_to_remove(w):\n",
    "    return any([\n",
    "                  w.is_bracket\n",
    "                , w.is_punct\n",
    "                , w.is_quote\n",
    "                , w.is_stop\n",
    "                , w.is_space\n",
    "                , w.text in [\"\\n\"]\n",
    "                , not w.i and w.text == \"RT\"\n",
    "                ])\n",
    "\n",
    "def format_date(d):\n",
    "    return int(mktime(datetime.strptime(d, \"%a %b %d %H:%M:%S +0000 %Y\").timetuple()))\n",
    "\n",
    "def buildStructure(raw_tweet):\n",
    "    tweet = dict(raw_tweet._json)\n",
    "    text_w_emojis = \" \".join([w.text for w in nlp(tweet[\"full_text\"]) if not caracters_to_remove(w)])\n",
    "    \n",
    "    clean_text = get_emoji_regexp().sub(\"\", text_w_emojis)\n",
    "    \n",
    "    structure = {\n",
    "        \"user_id\" : tweet[\"user\"][\"id_str\"],\n",
    "        \"screen_name\" : tweet[\"user\"][\"screen_name\"],\n",
    "        \"followers\" : tweet[\"user\"][\"followers_count\"],\n",
    "        \"retweet_count\" : tweet[\"retweet_count\"],\n",
    "        \"favorited\" : tweet[\"favorited\"],\n",
    "        \"created_at\": format_date(tweet[\"created_at\"]),\n",
    "        \"id\": tweet[\"id_str\"],\n",
    "        \"text\": clean_text,\n",
    "        \"hashtags\": [h[\"text\"] for h in tweet[\"entities\"][\"hashtags\"]],\n",
    "        \"user_mentions\": [m[\"screen_name\"] for m in tweet[\"entities\"][\"user_mentions\"]],\n",
    "        \"urls\": tweet[\"entities\"][\"urls\"],\n",
    "        \"type\": tweet[\"metadata\"][\"result_type\"],\n",
    "        \"retweet\": \"retweeted_status\" in tweet\n",
    "    }\n",
    "    \n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = api.search(q = \"#bitcoin\", lang = \"en\", count = 200, tweet_mode = \"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [buildStructure(t) for t in search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '1305183711168692225',\n",
       " 'screen_name': 'Monobody4',\n",
       " 'followers': 355,\n",
       " 'retweet_count': 2429,\n",
       " 'favorited': False,\n",
       " 'created_at': 1645073698,\n",
       " 'id': '1494128204306690050',\n",
       " 'text': '@BTC_Archive  BREAKING Colorado accept Bitcoin tax payments year',\n",
       " 'hashtags': ['Bitcoin'],\n",
       " 'user_mentions': ['BTC_Archive'],\n",
       " 'urls': [],\n",
       " 'type': 'recent',\n",
       " 'retweet': True}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos baixar os dados do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturando dados do Twitter\n",
    "tweets = {each._json[\"user\"][\"name\"] : each._json[\"text\"]  for each in api.search(q = \"#bitcoin\", lang = \"pt\", count = 200)}\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos criar uma função que realize o filtro de palavras que desejamos trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(word, cut_stop = True):\n",
    "    if word.is_stop and cut_stop:\n",
    "        return False\n",
    "    elif word.is_punct:\n",
    "        return False\n",
    "    elif word.suffix_ == \"…\":\n",
    "        return False\n",
    "    elif word.like_url:\n",
    "        return False\n",
    "    elif word.like_email:\n",
    "        return False\n",
    "    elif word.like_num:\n",
    "        return False\n",
    "    elif word.prefix_ == \"@\":\n",
    "        return False\n",
    "    elif word.text in [\" \", \"\\n\", \"\\n\\n\", \"...\", 'RT']:\n",
    "        return False\n",
    "    elif not word.text.isalnum():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o pré processamento das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento: Stop Words e Lemmatazing\n",
    "processeded = []\n",
    "# interando sobre cada tweet\n",
    "for user, tweet in tweets.items():\n",
    "    row = []\n",
    "    for word in nlp(tweet): # este é o pipeline\n",
    "        # filtrando as palavras\n",
    "        if word_filter(word):\n",
    "            # após selecionar as palavras, é adicionado o seu formato lematizado\n",
    "            lemm = nlp.vocab[word.text]\n",
    "            row.append(lemm.text)\n",
    "    print(f\"{user} : {row}\")\n",
    "    processeded.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processeded = []\n",
    "ner = []\n",
    "adj = []\n",
    "for each in tweets.values():\n",
    "    doc = nlp(each)\n",
    "    processeded.append([nlp.vocab[word.text].text for word in doc if word_filter(word)])\n",
    "    ner.append([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 19:53:25.219143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-16 19:53:25.219175: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load' from 'tensorflow' (/home/calixto/Documentos/botcryptowatcher/lib/python3.9/site-packages/tensorflow/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load' from 'tensorflow' (/home/calixto/Documentos/botcryptowatcher/lib/python3.9/site-packages/tensorflow/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tfds.as_dataframe(ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
