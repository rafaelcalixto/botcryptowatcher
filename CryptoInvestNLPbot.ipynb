{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialmente é realizando o load das bibliotecas necessárias para o processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 20:12:08.412738: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-10 20:12:08.412775: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-10 20:12:09.546881: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-10 20:12:09.546908: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-10 20:12:09.546926: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calixto-VirtualBox): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# core\n",
    "from json import load as jsload\n",
    "from warnings import catch_warnings, filterwarnings\n",
    "from random import shuffle\n",
    "from time import strftime, mktime\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from sqlite3 import connect\n",
    "# Captação dos dados\n",
    "from tweepy import OAuthHandler, API\n",
    "# NLP\n",
    "from spacy import load\n",
    "# Auxiliar\n",
    "from emoji import is_emoji\n",
    "from labels import mapping\n",
    "# Deep Learning\n",
    "from tez import Model\n",
    "from torch import sigmoid, no_grad, LongTensor, sort as torch_sort\n",
    "from torch.nn import Dropout, Linear, BCEWithLogitsLoss\n",
    "from transformers import SqueezeBertModel, SqueezeBertTokenizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a conexão com o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_tweets = \"CREATE TABLE IF NOT EXISTS tweets (tweet_id TEXT PRIMARY KEY, user_id TEXT NOT NULL, sentments TEXT, sent_score INTEGER NOT NULL, favorited INTEGER, retweets INTEGER, created_at INTEGER NOT NULL, text TEXT NOT NULL, hashtags TEXT, mentions TEXT, urls TEXT, type TEXT, retweet INTEGER, FOREIGN KEY (sentments) REFERENCES sentment (sentment_id));\"\n",
    "create_table_users = \"CREATE TABLE IF NOT EXISTS users (user_id TEXT PRIMARY KEY, screen_name TEXT, followers INTEGER);\"\n",
    "create_table_sentment = \"CREATE TABLE IF NOT EXISTS sentment (sentment_id INTEGER PRIMARY KEY, sentment_char TEXT);\"\n",
    "create_table_indexes = \"CREATE TABLE IF NOT EXISTS indexes (index_id INTEGER PRIMARY KEY, cryptocurrency TEXT NOT NULL, timestamp INTEGER NOT NULL);\"\n",
    "insert_table_tweets = \"INSERT INTO tweets(tweet_id, user_id, sentments, sent_score, favorited, retweets, created_at, text, hashtags, mentions, urls, type, retweet) VALUES ('{}', '{}', '{}', {}, {}, {}, {}, '{}', '{}', '{}', '{}', '{}', '{}')\"\n",
    "insert_table_users = \"INSERT INTO users(user_id, screen_name, followers) VALUES ('{}', '{}', {})\"\n",
    "insert_table_sentment = \"INSERT INTO sentment(sentment_id, sentment_char) VALUES ({}, '{}')\"\n",
    "\n",
    "try:\n",
    "    conn = connect(\".\\cryptobot_new_try.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(create_table_tweets)\n",
    "    cursor.execute(create_table_users)\n",
    "    cursor.execute(create_table_sentment)\n",
    "    cursor.execute(create_table_indexes)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o load modelo pré treinado spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloquei as chaves de acesso a minha conta no Twitter em um arquivo separado :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"key.json\") as file:\n",
    "    keys = jsload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(\n",
    "    consumer_key = keys[\"API key\"],\n",
    "    consumer_secret = keys[\"API secret key\"]\n",
    ")\n",
    "\n",
    "auth.set_access_token(\n",
    "    key = keys[\"Access token\"],\n",
    "    secret = keys[\"Access token secret\"]\n",
    ")\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o modelo treinado para identificação das emoções em textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        self.val_strategy = \"batch\"\n",
    "\n",
    "class EmotionClassifier(Model):\n",
    "    def __init__(self, num_train_steps, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = SqueezeBertModel.from_pretrained(\"squeezebert/squeezebert-uncased\")\n",
    "        self.bert_drop = Dropout(0.3)\n",
    "        self.out = Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, ids, mask, targets= None):\n",
    "        o_2 = self.bert(ids, attention_mask=mask)[\"pooler_output\"]\n",
    "        b_o = self.bert_drop(o_2)\n",
    "        output = self.out(b_o)\n",
    "        loss = self.loss(output, targets)\n",
    "        acc = self.monitor_metrics(output, targets)\n",
    "        return output, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = EmotionClassifier(int(43410 / 32 * 10), len(mapping))\n",
    "model.load(\"export/model.bin\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a função de identificação dos sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(text, topn=5):\n",
    "    final_classification = {}\n",
    "    tokenizer = SqueezeBertTokenizer.from_pretrained(\"squeezebert/squeezebert-uncased\", do_lower_case=True)\n",
    "    max_len = 35\n",
    "    with no_grad():\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        ids = LongTensor(ids).cpu().unsqueeze(0)\n",
    "        \n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        attention_mask = LongTensor(attention_mask).cpu().unsqueeze(0)\n",
    "        \n",
    "        output = model.forward(ids, attention_mask)[0]\n",
    "        output = sigmoid(output)\n",
    "        \n",
    "        probas, indices = torch_sort(output)\n",
    "        \n",
    "    probas = probas.cpu().numpy()[0][::-1]\n",
    "    indices = indices.cpu().numpy()[0][::-1]\n",
    "    \n",
    "    for i, p in zip(indices[:topn], probas[:topn]):\n",
    "        final_classification[mapping[i]] = p\n",
    "        \n",
    "    return final_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando função para estruturação das predições e tratamento dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tabela de pesos para cada sentimento\n",
    "sent_pesos = {\n",
    "    \"admiration\":8,\n",
    "    \"amusement\":4,\n",
    "    \"anger\":-7,\n",
    "    \"annoyance\":-9,\n",
    "    \"approval\":7,\n",
    "    \"caring\":-8,\n",
    "    \"confusion\":-5,\n",
    "    \"curiosity\":5,\n",
    "    \"desire\":9,\n",
    "    \"disappointment\":-10,\n",
    "    \"disapproval\":-9,\n",
    "    \"disgust\":-5,\n",
    "    \"embarrassment\":-6,\n",
    "    \"excitement\":10,\n",
    "    \"fear\":-8,\n",
    "    \"gratitude\":6,\n",
    "    \"grief\":-2,\n",
    "    \"joy\":1,\n",
    "    \"love\":2,\n",
    "    \"nervousness\":-4,\n",
    "    \"optimism\":10,\n",
    "    \"pride\":8,\n",
    "    \"realization\":3,\n",
    "    \"relief\":4,\n",
    "    \"remorse\":-10,\n",
    "    \"sadness\":-3,\n",
    "    \"surprise\":1,\n",
    "    \"neutral\":0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caracters_to_remove(w):\n",
    "    return any([\n",
    "                  w.is_bracket\n",
    "                , w.is_punct\n",
    "                , w.is_quote\n",
    "                , w.is_stop\n",
    "                , w.is_space\n",
    "                , w.text in [\"\\n\"]\n",
    "                , not w.i and w.text == \"RT\"\n",
    "                , is_emoji(w.text)\n",
    "                ])\n",
    "\n",
    "def format_date(d):\n",
    "    return int(mktime(datetime.strptime(d, \"%a %b %d %H:%M:%S +0000 %Y\").timetuple()))\n",
    "\n",
    "def buildStructure(raw_tweet):\n",
    "    tweet = dict(raw_tweet._json)\n",
    "    clean_text = \" \".join([w.text for w in nlp(tweet[\"full_text\"]) if not caracters_to_remove(w)])\n",
    "    \n",
    "    sentments = score_sentence(clean_text)\n",
    "    \n",
    "    structure = {\n",
    "        \"user_id\" : tweet[\"user\"][\"id_str\"],\n",
    "        \"screen_name\" : tweet[\"user\"][\"screen_name\"],\n",
    "        \"followers\" : tweet[\"user\"][\"followers_count\"],\n",
    "        \"retweet_count\" : tweet[\"retweet_count\"],\n",
    "        \"favorited\" : tweet[\"favorited\"],\n",
    "        \"created_at\": format_date(tweet[\"created_at\"]),\n",
    "        \"id\": tweet[\"id_str\"],\n",
    "        \"text\": clean_text,\n",
    "        \"hashtags\": [h[\"text\"] for h in tweet[\"entities\"][\"hashtags\"]],\n",
    "        \"user_mentions\": [m[\"screen_name\"] for m in tweet[\"entities\"][\"user_mentions\"]],\n",
    "        \"urls\": tweet[\"entities\"][\"urls\"],\n",
    "        \"type\": tweet[\"metadata\"][\"result_type\"],\n",
    "        \"retweet\": \"retweeted_status\" in tweet,\n",
    "        \"sentments\": sentments,\n",
    "        \"sent_score\": sum([sent_pesos[k] * v for k, v in sentments.items()])\n",
    "    }\n",
    "    \n",
    "    return structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### É criada uma função anônima para realizar o cálculo do índice para a predição do valor de cada criptomoeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lambda Nf, Nrt, S, Dtc : ((1+(Nf*0.01)+(Nrt*0.1))*(S*0.1))/(1+((Dtc-int(time()))*0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do loop para captação dos dados no Twitter e cálculo do índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: users.user_id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tweets:\n\u001b[1;32m     13\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(insert_table_tweets\u001b[38;5;241m.\u001b[39mformat(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()), t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_score\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorited\u001b[39m\u001b[38;5;124m\"\u001b[39m]), t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretweet_count\u001b[39m\u001b[38;5;124m\"\u001b[39m], t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m], t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_mentions\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([u[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m\"\u001b[39m]]), t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mint\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretweet\u001b[39m\u001b[38;5;124m\"\u001b[39m])))\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_table_users\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscreen_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfollowers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([prediction(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowers\u001b[39m\u001b[38;5;124m\"\u001b[39m], t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretweet_count\u001b[39m\u001b[38;5;124m\"\u001b[39m], t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_score\u001b[39m\u001b[38;5;124m\"\u001b[39m], t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tweets])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(crypto, index, time())\n",
      "\u001b[0;31mIntegrityError\u001b[0m: UNIQUE constraint failed: users.user_id"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "while True:\n",
    "    crypto = [\"bitcoin\", \"ethereum\", \"binance coin\"][step % 3]\n",
    "    search = api.search_tweets(\n",
    "                                q = crypto, \n",
    "                                lang = \"en\", \n",
    "                                count = 200, \n",
    "                                result_type = \"recent\", \n",
    "                                tweet_mode = \"extended\"\n",
    "            )\n",
    "    tweets = [buildStructure(t) for t in search]\n",
    "    for t in tweets:\n",
    "        cursor.execute(insert_table_tweets.format(t[\"id\"], t[\"user_id\"], \" \".join(t[\"sentments\"].keys()), t[\"sent_score\"], int(t[\"favorited\"]), t[\"retweet_count\"], t[\"created_at\"], t[\"text\"], \", \".join(t[\"hashtags\"]), \", \".join(t[\"user_mentions\"]), \", \".join([u[\"url\"] for u in t[\"urls\"]]), t[\"type\"], int(t[\"retweet\"])))\n",
    "        cursor.execute(insert_table_users.format(t[\"user_id\"], t[\"screen_name\"], t[\"followers\"]))\n",
    "    index = sum([prediction(t[\"followers\"], t[\"retweet_count\"], t[\"sent_score\"], t[\"created_at\"]) for t in tweets])\n",
    "    print(crypto, index, time())\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
