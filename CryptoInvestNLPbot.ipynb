{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialmente é realizando o load das bibliotecas necessárias para o processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "from json import load as jsload\n",
    "from warnings import catch_warnings, filterwarnings\n",
    "from random import shuffle\n",
    "from time import strftime, mktime\n",
    "from datetime import datetime\n",
    "# Captação dos dados\n",
    "from tweepy import OAuthHandler, API\n",
    "# NLP\n",
    "from spacy import load\n",
    "# Auxiliar\n",
    "from emoji import get_emoji_regexp\n",
    "# Deep Learning\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o load modelo pré treinado spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloquei as chaves de acesso a minha conta no Twitter em um arquivo separado :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"key.json\") as file:\n",
    "    keys = jsload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(\n",
    "    consumer_key = keys[\"API key\"],\n",
    "    consumer_secret = keys[\"API secret key\"]\n",
    ")\n",
    "\n",
    "auth.set_access_token(\n",
    "    key = keys[\"Access token\"],\n",
    "    secret = keys[\"Access token secret\"]\n",
    ")\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o modelo treinado para identificação das emoções em textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        self.val_strategy = \"batch\"\n",
    "\n",
    "class EmotionClassifier(tez.Model):\n",
    "    def __init__(self, num_train_steps, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.SqueezeBertModel.from_pretrained(\"squeezebert/squeezebert-uncased\")\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, num_classes)\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.config = config()\n",
    "        \n",
    "    def fetch_optimizer(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            }\n",
    "        ]\n",
    "        opt = AdamW(optimizer_parameters, lr=3e-5)\n",
    "        return opt\n",
    "    \n",
    "    def fetch_scheduler(self):\n",
    "        sch = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\n",
    "        )\n",
    "        return sch\n",
    "    \n",
    "    def loss(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return None\n",
    "        return nn.BCEWithLogitsLoss()(outputs, targets.float())\n",
    "    \n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return {}\n",
    "        \n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        \n",
    "        fpr_micro, tpr_micro, _ = metrics.roc_curve(targets.ravel(), outputs.ravel())\n",
    "        auc_micro = metrics.auc(fpr_micro, tpr_micro)\n",
    "        return {\"auc\": auc_micro}\n",
    "    \n",
    "    def forward(self, ids, mask, targets= None):\n",
    "        o_2 = self.bert(ids, attention_mask=mask)[\"pooler_output\"]\n",
    "        b_o = self.bert_drop(o_2)\n",
    "        output = self.out(b_o)\n",
    "        loss = self.loss(output, targets)\n",
    "        acc = self.monitor_metrics(output, targets)\n",
    "        return output, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando função para estruturação das predições e tratamento dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caracters_to_remove(w):\n",
    "    return any([\n",
    "                  w.is_bracket\n",
    "                , w.is_punct\n",
    "                , w.is_quote\n",
    "                , w.is_stop\n",
    "                , w.is_space\n",
    "                , w.text in [\"\\n\"]\n",
    "                , not w.i and w.text == \"RT\"\n",
    "                ])\n",
    "\n",
    "def format_date(d):\n",
    "    return int(mktime(datetime.strptime(d, \"%a %b %d %H:%M:%S +0000 %Y\").timetuple()))\n",
    "\n",
    "def buildStructure(raw_tweet):\n",
    "    tweet = dict(raw_tweet._json)\n",
    "    text_w_emojis = \" \".join([w.text for w in nlp(tweet[\"full_text\"]) if not caracters_to_remove(w)])\n",
    "    \n",
    "    clean_text = get_emoji_regexp().sub(\"\", text_w_emojis)\n",
    "    \n",
    "    structure = {\n",
    "        \"user_id\" : tweet[\"user\"][\"id_str\"],\n",
    "        \"screen_name\" : tweet[\"user\"][\"screen_name\"],\n",
    "        \"followers\" : tweet[\"user\"][\"followers_count\"],\n",
    "        \"retweet_count\" : tweet[\"retweet_count\"],\n",
    "        \"favorited\" : tweet[\"favorited\"],\n",
    "        \"created_at\": format_date(tweet[\"created_at\"]),\n",
    "        \"id\": tweet[\"id_str\"],\n",
    "        \"text\": clean_text,\n",
    "        \"hashtags\": [h[\"text\"] for h in tweet[\"entities\"][\"hashtags\"]],\n",
    "        \"user_mentions\": [m[\"screen_name\"] for m in tweet[\"entities\"][\"user_mentions\"]],\n",
    "        \"urls\": tweet[\"entities\"][\"urls\"],\n",
    "        \"type\": tweet[\"metadata\"][\"result_type\"],\n",
    "        \"retweet\": \"retweeted_status\" in tweet\n",
    "    }\n",
    "    \n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = api.search(q = \"#bitcoin\", lang = \"en\", count = 200, tweet_mode = \"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [buildStructure(t) for t in search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '1305183711168692225',\n",
       " 'screen_name': 'Monobody4',\n",
       " 'followers': 355,\n",
       " 'retweet_count': 2429,\n",
       " 'favorited': False,\n",
       " 'created_at': 1645073698,\n",
       " 'id': '1494128204306690050',\n",
       " 'text': '@BTC_Archive  BREAKING Colorado accept Bitcoin tax payments year',\n",
       " 'hashtags': ['Bitcoin'],\n",
       " 'user_mentions': ['BTC_Archive'],\n",
       " 'urls': [],\n",
       " 'type': 'recent',\n",
       " 'retweet': True}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos baixar os dados do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturando dados do Twitter\n",
    "tweets = {each._json[\"user\"][\"name\"] : each._json[\"text\"]  for each in api.search(q = \"#bitcoin\", lang = \"pt\", count = 200)}\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos criar uma função que realize o filtro de palavras que desejamos trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(word, cut_stop = True):\n",
    "    if word.is_stop and cut_stop:\n",
    "        return False\n",
    "    elif word.is_punct:\n",
    "        return False\n",
    "    elif word.suffix_ == \"…\":\n",
    "        return False\n",
    "    elif word.like_url:\n",
    "        return False\n",
    "    elif word.like_email:\n",
    "        return False\n",
    "    elif word.like_num:\n",
    "        return False\n",
    "    elif word.prefix_ == \"@\":\n",
    "        return False\n",
    "    elif word.text in [\" \", \"\\n\", \"\\n\\n\", \"...\", 'RT']:\n",
    "        return False\n",
    "    elif not word.text.isalnum():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o pré processamento das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento: Stop Words e Lemmatazing\n",
    "processeded = []\n",
    "# interando sobre cada tweet\n",
    "for user, tweet in tweets.items():\n",
    "    row = []\n",
    "    for word in nlp(tweet): # este é o pipeline\n",
    "        # filtrando as palavras\n",
    "        if word_filter(word):\n",
    "            # após selecionar as palavras, é adicionado o seu formato lematizado\n",
    "            lemm = nlp.vocab[word.text]\n",
    "            row.append(lemm.text)\n",
    "    print(f\"{user} : {row}\")\n",
    "    processeded.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processeded = []\n",
    "ner = []\n",
    "adj = []\n",
    "for each in tweets.values():\n",
    "    doc = nlp(each)\n",
    "    processeded.append([nlp.vocab[word.text].text for word in doc if word_filter(word)])\n",
    "    ner.append([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 19:53:25.219143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-16 19:53:25.219175: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load' from 'tensorflow' (/home/calixto/Documentos/botcryptowatcher/lib/python3.9/site-packages/tensorflow/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load' from 'tensorflow' (/home/calixto/Documentos/botcryptowatcher/lib/python3.9/site-packages/tensorflow/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tfds.as_dataframe(ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
